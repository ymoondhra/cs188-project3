{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph \n",
    "from datetime import date\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random \n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_1 = \"training_dataset.csv\"\n",
    "csv_path_2 = \"score.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. First, let's understand our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path_1) # load the pandas dataframe\n",
    "df_score = pd.read_csv(csv_path_2)\n",
    "initial_cols_to_drop = [\"Unnamed: 0\",\"Unnamed: 0.1\", \"period\", \"test\", \"recent_date\", \"date\"] \n",
    "for col_name in initial_cols_to_drop: # drops columns that aren't supposed to be in dataset\n",
    "    try:\n",
    "        df = df.drop(columns=[col_name])\n",
    "        df_score = df_score.drop(columns=[col_name])\n",
    "    except:\n",
    "        continue\n",
    "#df = df.rename(columns={\"Unnamed: 0.1\": \"TODO_FIND_COLUMN_NAME_2\"})\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts date from csv to a python datetime object making it easier to work with\n",
    "def convert_dates(df):\n",
    "    dates_columns = ['most_recent_load_date', 'first_load_date', 'load_day', 'dt']\n",
    "    for col_name in dates_columns:\n",
    "        try:\n",
    "            df[col_name] = pd.to_datetime(df[col_name], format='%Y-%m-%d')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "convert_dates(df)\n",
    "convert_dates(df_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['most_recent_load_date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loads75 = df.total_loads.quantile(0.75) # finds 75th percentile of loads\n",
    "most_recent_load_date75 = df.most_recent_load_date.quantile(0.75) # finds 75th percentile of most recent load date\n",
    "\n",
    "print(total_loads75)\n",
    "print(most_recent_load_date75)\n",
    "\n",
    "\n",
    "# Manual Check\n",
    "# sorted_dts = sorted(list(df.most_recent_load_date))\n",
    "# quartile_estimate_index = int(len(sorted_dts)*0.75)\n",
    "# print(\"SORTED INDEX\", sorted_dts[quartile_estimate_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_labels = {\"label\": {}}\n",
    "num_days_worked_dict = {}\n",
    "\n",
    "for index, row in df.iterrows(): # changes the labels in the label columns\n",
    "    # checks if the load and most recent load date are in the 75th percentile\n",
    "    if row[\"total_loads\"] >= total_loads75 and row[\"most_recent_load_date\"] >= most_recent_load_date75:\n",
    "        df.at[index, \"label\"] = 1\n",
    "    else:\n",
    "        df.at[index, \"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dt\"].groupby([df[\"dt\"].dt.year, df[\"dt\"].dt.month]).count().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby(\"id_driver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO: dt, weekday, year, id_carrier_number, dim_preferred_lanes, load_day, loads\n",
    "new_arr = []\n",
    "for key, group in groups:\n",
    "    group.sort_values(by=\"load_day\", ascending=False, inplace=True)\n",
    "    temp_arr = []\n",
    "    temp_arr.append(key)\n",
    "    \n",
    "    if group[\"dim_carrier_type\"].nunique() == 2:\n",
    "        temp_arr.append(\"Both\")\n",
    "    elif group[\"dim_carrier_type\"].nunique() == 0:\n",
    "        temp_arr.append(None)\n",
    "    else:\n",
    "        temp_arr.append((group[\"dim_carrier_type\"].iloc[0]))\n",
    "    \n",
    "    \n",
    "    idxmax_cols = [\"dim_carrier_company_name\", \"home_base_city\", \"home_base_state\", \n",
    "                   \"carrier_trucks\", \"signup_source\", \"ts_signup\", \"ts_first_approved\",\n",
    "                  \"days_signup_to_approval\"]\n",
    "    \n",
    "    for col in idxmax_cols:\n",
    "        try:\n",
    "            temp_arr.append(group[col].value_counts().dropna(how=\"any\").idxmax())\n",
    "        except:\n",
    "            temp_arr.append(None)\n",
    "    \n",
    "    try:\n",
    "        temp_arr.append(group[\"num_trucks\"].dropna(how=\"any\").mean())\n",
    "    except:\n",
    "        temp_arr.append(None)\n",
    "        \n",
    "    iloc_cols = [\"interested_in_drayage\", \"port_qualified\", \"driver_with_twic\", \n",
    "                 \"first_load_date\", \"most_recent_load_date\", \"marketplace_loads_otr\", \n",
    "                 \"marketplace_loads_atlas\", \"marketplace_loads\", \"brokerage_loads_otr\",\n",
    "                 \"brokerage_loads_atlas\", \"brokerage_loads\", \"total_loads\"]\n",
    "    for col in iloc_cols:\n",
    "        try:\n",
    "            temp_arr.append(group[col].dropna(how=\"any\").iloc[0])\n",
    "        except:\n",
    "            temp_arr.append(None)\n",
    "            \n",
    "    temp_arr.append(group[\"label\"].value_counts().dropna(how=\"any\").idxmax())\n",
    "            \n",
    "    temp_arr.append(group.shape[0])\n",
    "    \n",
    "    new_arr.append(np.array(temp_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"id_driver\", \"dim_carrier_type\", \"dim_carrier_company_name\", \"home_base_city\", \"home_base_state\", \n",
    "                \"carrier_trucks\", \"signup_source\", \"ts_signup\", \"ts_first_approved\",\n",
    "                \"days_signup_to_approval\", \"num_trucks\", \"interested_in_drayage\", \n",
    "                \"port_qualified\", \"driver_with_twic\", \n",
    "                \"first_load_date\", \"most_recent_load_date\", \"marketplace_loads_otr\", \n",
    "                \"marketplace_loads_atlas\", \"marketplace_loads\", \"brokerage_loads_otr\",\n",
    "                \"brokerage_loads_atlas\", \"brokerage_loads\", \"total_loads\", \"num_trips_made\", \"label\"]\n",
    "\n",
    "df = pd.DataFrame(np.array(new_arr), columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert = [\"id_driver\", \"days_signup_to_approval\", \"marketplace_loads_otr\", \n",
    "               \"marketplace_loads_atlas\", \"marketplace_loads\", \"brokerage_loads_otr\",\n",
    "               \"brokerage_loads_atlas\", \"brokerage_loads\", \"total_loads\", \"num_trips_made\", \"label\",\n",
    "               \"num_trucks\", \"dim_carrier_type\", \"dim_carrier_company_name\", \"home_base_city\", \"home_base_state\",\n",
    "               \"interested_in_drayage\", \"port_qualified\", \"signup_source\", \"driver_with_twic\"]\n",
    "for col in convert:\n",
    "    df[col] = df[col].convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_score.groupby(\"id_driver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NO: dt, weekday, year, id_carrier_number, dim_preferred_lanes, load_day, loads\n",
    "# new_arr = []\n",
    "# for key, group in groups:\n",
    "#     group.sort_values(by=\"load_day\", ascending=False, inplace=True)\n",
    "#     temp_arr = []\n",
    "#     temp_arr.append(key)\n",
    "    \n",
    "#     if group[\"dim_carrier_type\"].nunique() == 2:\n",
    "#         temp_arr.append(\"Both\")\n",
    "#     elif group[\"dim_carrier_type\"].nunique() == 0:\n",
    "#         temp_arr.append(None)\n",
    "#     else:\n",
    "#         temp_arr.append((group[\"dim_carrier_type\"].iloc[0]))\n",
    "    \n",
    "    \n",
    "#     idxmax_cols = [\"dim_carrier_company_name\", \"home_base_city\", \"home_base_state\", \n",
    "#                    \"carrier_trucks\", \"signup_source\", \"ts_signup\", \"ts_first_approved\",\n",
    "#                   \"days_signup_to_approval\"]\n",
    "    \n",
    "#     for col in idxmax_cols:\n",
    "#         try:\n",
    "#             temp_arr.append(group[col].value_counts().dropna(how=\"any\").idxmax())\n",
    "#         except:\n",
    "#             temp_arr.append(None)\n",
    "    \n",
    "#     try:\n",
    "#         temp_arr.append(group[\"num_trucks\"].dropna(how=\"any\").mean())\n",
    "#     except:\n",
    "#         temp_arr.append(None)\n",
    "        \n",
    "#     iloc_cols = [\"interested_in_drayage\", \"port_qualified\", \"driver_with_twic\", \n",
    "#                  \"first_load_date\", \"load_day\", \"marketplace_loads_otr\", \n",
    "#                  \"marketplace_loads_atlas\", \"marketplace_loads\", \"brokerage_loads_otr\",\n",
    "#                  \"brokerage_loads_atlas\", \"brokerage_loads\"]\n",
    "#     for col in iloc_cols:\n",
    "#         try:\n",
    "#             temp_arr.append(group[col].dropna(how=\"any\").iloc[0])\n",
    "#         except:\n",
    "#             temp_arr.append(None)\n",
    "            \n",
    "#     temp_arr.append(group.shape[0])\n",
    "    \n",
    "#     new_arr.append(np.array(temp_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = [\"id_driver\", \"dim_carrier_type\", \"dim_carrier_company_name\", \"home_base_city\", \"home_base_state\", \n",
    "#                 \"carrier_trucks\", \"signup_source\", \"ts_signup\", \"ts_first_approved\",\n",
    "#                 \"days_signup_to_approval\", \"num_trucks\", \"interested_in_drayage\", \n",
    "#                 \"port_qualified\", \"driver_with_twic\", \n",
    "#                 \"first_load_date\", \"most_recent_load_date\", \"marketplace_loads_otr\", \n",
    "#                 \"marketplace_loads_atlas\", \"marketplace_loads\", \"brokerage_loads_otr\",\n",
    "#                 \"brokerage_loads_atlas\", \"brokerage_loads\", \"num_trips_made\"]\n",
    "\n",
    "# df_score = pd.DataFrame(np.array(new_arr), columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert = [\"id_driver\", \"days_signup_to_approval\", \"marketplace_loads_otr\", \n",
    "#                \"marketplace_loads_atlas\", \"marketplace_loads\", \"brokerage_loads_otr\",\n",
    "#                \"brokerage_loads_atlas\", \"brokerage_loads\", \"num_trips_made\",\n",
    "#                \"num_trucks\", \"dim_carrier_type\", \"dim_carrier_company_name\", \"home_base_city\", \"home_base_state\",\n",
    "#                \"interested_in_drayage\", \"port_qualified\", \"signup_source\", \"driver_with_twic\"]\n",
    "# for col in convert:\n",
    "#     df_score[col] = df_score[col].convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO: dt, weekday, year, id_carrier_number, dim_preferred_lanes, load_day, loads\n",
    "new_dict = {}\n",
    "most_recent_date_arr = []\n",
    "num_trips_arr = []\n",
    "for key, group in groups:\n",
    "    group.sort_values(by=\"load_day\", ascending=False, inplace=True)\n",
    "    if key not in new_dict:\n",
    "        try:\n",
    "            new_dict[key] = (group[\"load_day\"].dropna(how=\"any\").iloc[0], group.shape[0])\n",
    "        except:\n",
    "            new_dict[key] = None\n",
    "\n",
    "for index, row in df_score.iterrows():\n",
    "    most_recent_date_arr.append(new_dict[row[\"id_driver\"]][0])\n",
    "    num_trips_arr.append(new_dict[row[\"id_driver\"]][1])\n",
    "df_score[\"most_recent_load_date\"] = np.array(most_recent_date_arr)\n",
    "df_score[\"num_trips_made\"] = np.array(num_trips_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_score[\"num_trips_made\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NO: dt, weekday, year, id_carrier_number, dim_preferred_lanes, load_day, loads\n",
    "\n",
    "\n",
    "\n",
    "# [\"id_driver\", \"dim_carrier_type\", \"dim_carrier_company_name\", \"home_base_city\", \"home_base_state\", \n",
    "#                 \"carrier_trucks\", \"signup_source\", \"ts_signup\", \"ts_first_approved\",\n",
    "#                 \"days_signup_to_approval\", \"num_trucks\", \"interested_in_drayage\", \n",
    "#                 \"port_qualified\", \"driver_with_twic\", \n",
    "#                 \"first_load_date\", \"most_recent_load_date\", \"marketplace_loads_otr\", \n",
    "#                 \"marketplace_loads_atlas\", \"marketplace_loads\", \"brokerage_loads_otr\",\n",
    "#                 \"brokerage_loads_atlas\", \"brokerage_loads\", \"total_loads\", \"num_trips_made\", \"label\"]\n",
    "\n",
    "drop_cols = [\"dt\", \"weekday\", \"year\", \"id_carrier_number\", \"dim_preferred_lanes\", \"load_day\", \"loads\"]\n",
    "for col in drop_cols:\n",
    "    try:\n",
    "        df_score = df_score.drop(columns=[col])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "for col_name in (list(df.columns.values)): # prints all the correlation matrices corresponding to each feature\n",
    "    try:\n",
    "        print(col_name)\n",
    "        display(corr_matrix[col_name].sort_values(ascending=False))\n",
    "        print('---------------------------------------------------------------------')\n",
    "    except:\n",
    "        print(\"{} is not of type integer\".format(col_name))\n",
    "        print('---------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Also year and TODO_FIND_COLUMN_NAME_2 and year are highly correlated and have a similar impact on label, so we could drop one? \n",
    "\n",
    "Is there really a need for brokerage_loads when it is so highly correlated to brokerage_loads_otr due to the vast majority of shipments being delivered over-the-road as compared to via ATLAS? \n",
    "\n",
    "I have the same question about total_loads due to the vast majority of loads being brokerage loads...\n",
    "\n",
    "What's the point of having both year and date?\n",
    "\n",
    "We can remove the id_carrier_number column from this dataset as it is not relevant to predicting a label of 0 or 1 (When trying to find high performing drivers, we need to know their carrier number, so we can extract the id_carrier_number column for now...)\n",
    "\n",
    "We could one-hot-encode sign-up source and see its effect on labels.\n",
    "\n",
    "We can remove the ts_first_approved column because the date of approval shouldn't matter that much but instead the days_signup_to_approval matter.\n",
    "\n",
    "dim_preferred_lanes only has a few values so we can either remove the column or impute values.\n",
    "\n",
    "Also first_load_date, most_recent_load_date and load_day shouldn't matter much. Instead we can have values such as: number of days doing the job = most_recent_load_date - first_load_date\n",
    "AND\n",
    "days_from_last_load_to_today = todays_date - most_recent_load_date\n",
    "\n",
    "There are also a couple other features we need to impute.\n",
    "\n",
    "Also, only people that are port qualified can provide drayage services, so we should create a field called qualified_and_interest_in_drayage which is only 1 (yes) when interested_in_drayage = \"yes\" and port_qualified = \"yes\". We can also cross these features..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Feature Extraction Plan and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"location\"] = list(zip(df[\"home_base_city\"], df[\"home_base_state\"]))# feature cross to get (city, state) tuple\n",
    "# # feature cross for interested in drayage and port qualified\n",
    "# df[\"drayage_interested_port_qualified\"] = list(zip(df[\"interested_in_drayage\"], df[\"port_qualified\"]))\n",
    "# display(df[\"location\"])\n",
    "# display(df[\"drayage_interested_port_qualified\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drayage_feature_cross(df):\n",
    "    loc_cross = list(zip(df[\"home_base_city\"], df[\"home_base_state\"]))# feature cross to get (city, state) tuple\n",
    "    # feature cross for interested in drayage and port qualified\n",
    "    drayage_cross = list(zip(df[\"interested_in_drayage\"], df[\"port_qualified\"]))\n",
    "\n",
    "    drayage_arr = []\n",
    "    for list_item in drayage_cross:\n",
    "        if list_item[0] == \"yes\" and list_item[1] == \"yes\":\n",
    "            drayage_arr.append(\"000001\")\n",
    "        if list_item[0] == \"yes\" and list_item[1] == \"no\":\n",
    "            drayage_arr.append(\"000010\")\n",
    "        if list_item[0] == \"no\" and list_item[1] == \"yes\":\n",
    "            drayage_arr.append(\"000100\")\n",
    "        if list_item[0] == \"no\" and list_item[1] == \"no\":\n",
    "            drayage_arr.append(\"001000\")\n",
    "        if list_item[0] == \"not specified\" and list_item[1] == \"yes\":\n",
    "            drayage_arr.append(\"010000\")\n",
    "        if list_item[0] == \"not specified\" and list_item[1] == \"no\":\n",
    "            drayage_arr.append(\"100000\")\n",
    "\n",
    "    df[\"drayage_interested_port_qualified\"] = np.array(drayage_arr)\n",
    "    display(df[\"drayage_interested_port_qualified\"])\n",
    "\n",
    "drayage_feature_cross(df)\n",
    "drayage_feature_cross(df_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_driver_number_col = np.array(df[\"id_driver\"]) # extract id_driver column\n",
    "id_driver_number_col_score = np.array(df_score[\"id_driver\"]) # extract id_driver column\n",
    "\n",
    "drop_cols = [\"id_driver\", \"home_base_city\",\n",
    "             \"home_base_state\", \"interested_in_drayage\", \"port_qualified\", \n",
    "             \"ts_signup\", \"ts_first_approved\"]\n",
    "for col in drop_cols:\n",
    "    try:\n",
    "        df = df.drop(columns = [col]) # drop columns that don't affect the label value by much\n",
    "        df_score = df_score.drop(columns = [col]) # drop columns that don't affect the label value by much\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {}\n",
    "for index, row in df.iterrows():\n",
    "    names[row[\"dim_carrier_company_name\"]] = int(names.get(row[\"dim_carrier_company_name\"], 0) + 1)\n",
    "listo = list(names.items())\n",
    "listo.sort(reverse=True, key=lambda x: int(x[1]))\n",
    "#listo[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listo = listo[:50]\n",
    "count_50 = sum([x[1] for x in listo])\n",
    "print(\"# 50: \", count_50)\n",
    "print(\"Percentage 50: \", count_50/len(names))\n",
    "\n",
    "names_arr = [tuples[0] for tuples in listo]\n",
    "print(names_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketize(df):\n",
    "    days_worked = []\n",
    "    for index, row in df.iterrows(): # bucketize the most frequent dim_carrier_company names, \n",
    "                                     # put less frequent names in a single bucket\n",
    "        try:\n",
    "            if row[\"dim_carrier_company_name\"] not in names_arr:\n",
    "                df.at[index, \"dim_carrier_company_name\"] = \"Other\"\n",
    "        except:\n",
    "            df.at[index, \"dim_carrier_company_name\"] = \"Other\"\n",
    "\n",
    "        # find number of days driver has worked\n",
    "        if row[\"most_recent_load_date\"] != np.nan and row[\"first_load_date\"] != np.nan:\n",
    "            days_worked.append((row[\"most_recent_load_date\"] - row[\"first_load_date\"]).days)\n",
    "        else:\n",
    "            days_worked.append(None)\n",
    "    df[\"days_tenured\"] = np.array(days_worked)\n",
    "\n",
    "bucketize(df)\n",
    "bucketize(df_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total_loads for df_score in order to find labels\n",
    "total_loads = []\n",
    "for index, row in df_score.iterrows():\n",
    "    total_loads.append(row[\"marketplace_loads\"] + row[\"brokerage_loads\"])\n",
    "df_score[\"total_loads\"] = np.array(total_loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loads_score_75 = df_score.total_loads.quantile(0.75) # finds 75th percentile of loads\n",
    "most_recent_load_date_score_75 = df_score.most_recent_load_date.quantile(0.75) # finds 75th percentile of most recent load date\n",
    "\n",
    "print(total_loads_score_75)\n",
    "print(most_recent_load_date_score_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_labels = []\n",
    "for index, row in df_score.iterrows(): # changes the labels in the label columns\n",
    "    # checks if the load and most recent load date are in the 75th percentile\n",
    "    if row[\"total_loads\"] >= total_loads_score_75 and row[\"most_recent_load_date\"] >= most_recent_load_date_score_75:\n",
    "        score_labels.append(1)\n",
    "    else:\n",
    "        score_labels.append(0)\n",
    "print(len(score_labels))\n",
    "print(score_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"most_recent_load_date\", \"first_load_date\", \"weekday\", \"load_day\", \"total_loads\"]\n",
    "for col in cols:\n",
    "    try:\n",
    "        df = df.drop(columns=[col])\n",
    "        df_score = df_score.drop(columns=[col])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled = df.drop(columns=[\"label\"])\n",
    "labels = df[\"label\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_unlabeled.columns.values)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = df_score[cols]\n",
    "\n",
    "df_score['num_trucks'] = pd.to_numeric(df_score['num_trucks'], errors='coerce')\n",
    "df_score['days_signup_to_approval'] = pd.to_numeric(df_score['days_signup_to_approval'], errors='coerce')\n",
    "\n",
    "convert = [\"id_driver\", \"days_signup_to_approval\", \"marketplace_loads_otr\", \n",
    "               \"marketplace_loads_atlas\", \"marketplace_loads\", \"brokerage_loads_otr\",\n",
    "               \"brokerage_loads_atlas\", \"brokerage_loads\", \"num_trips_made\",\n",
    "               \"num_trucks\", \"dim_carrier_type\", \"dim_carrier_company_name\",\n",
    "               \"interested_in_drayage\", \"port_qualified\", \"signup_source\", \"driver_with_twic\"]\n",
    "for col in convert:\n",
    "    try:\n",
    "        df_score[col] = df_score[col].convert_dtypes()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_unlabeled, df_score])\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in convert:\n",
    "    try:\n",
    "        df_concat[col] = df_concat[col].convert_dtypes()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop = [\"dim_carrier_type\", \"dim_carrier_company_name\", \"carrier_trucks\", \n",
    "#                                 \"signup_source\", \"driver_with_twic\", \"drayage_interested_port_qualified\"]\n",
    "# df_concat = df_concat.drop(columns=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer()\n",
    "categorical_features_one_hot = [\"dim_carrier_type\", \"dim_carrier_company_name\", \"carrier_trucks\", \n",
    "                                \"signup_source\", \"driver_with_twic\"]\n",
    "\n",
    "df_num = df_concat.drop(columns=categorical_features_one_hot)\n",
    "# df_num = df_concat\n",
    "numerical_features = list(df_num)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_features),\n",
    "        (\"cat\", OneHotEncoder(sparse=False), categorical_features_one_hot), #sparse=False\n",
    "    ])\n",
    "df_prepared = full_pipeline.fit_transform(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_prepared[:5291]\n",
    "y = labels\n",
    "score_y = df_prepared[5291:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "lr_predicted = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, lr_predicted)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_test, lr_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_new = sm.add_constant(X_train)\n",
    "toyregr_sm = sm.OLS(y_train.astype(float), X_new.astype(float))\n",
    "results_sm = toyregr_sm.fit()\n",
    "\n",
    "print(results_sm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y_temp = y_train.astype('int')\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_temp)\n",
    "log_predicted = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, log_predicted)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, log_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95) # Create an instance of PCA model\n",
    "pca.fit(X_train) # Fit X_train to PCA\n",
    "X_train = pca.transform(X_train) # transform training data\n",
    "X_test = pca.transform(X_test) # transform test data\n",
    "print(pca.explained_variance_)\n",
    "print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ensemble AKA Robert's BS pls mercy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# assuming we have X_train,X_test,y_train,y_test at this time\n",
    "# I first run Random Forest using random hard coded settings to get a baseline\n",
    "rf = RandomForestRegressor(n_estimators=80,max_depth=7,max_features=3)\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "test_score = r2_score(y_test,y_pred)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "# I then use RandomizedSearchCV to find the optimal hyperparameters\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(5, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 7, 10, 20]\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I then output the r2 score again as a sanity check to verify that my RanomdizedSearchCV actually did find the best settings\n",
    "rf = RandomForestRegressor(n_estimators=700,max_depth=47,max_features='auto',min_samples_split=2,min_samples_leaf=2,bootstrap=True)\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "test_score = r2_score(y_test,y_pred)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then use the hyperparameters we found from the RandomizedSearchCV to do a second more thorough check around that range\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [40, 45, 50, 55, 60],\n",
    "    'max_features': [2, 5, 7, 10, 12],\n",
    "    'min_samples_leaf': [2, 3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6],\n",
    "    'n_estimators': [100, 200, 500, 700, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then output the results using the optimal hyperparameters to check that our model has improved\n",
    "rf = RandomForestRegressor(n_estimators=500,max_depth=40,max_features=7,min_samples_split=4,min_samples_leaf=2,bootstrap=True)\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "test_score = r2_score(y_test,y_pred)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost using the same settings\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "ab = AdaBoostClassifier(base_estimator=dt,learning_rate=1,n_estimators=50)\n",
    "ab.fit(X_train,y_train)\n",
    "y_pred = ab.predict(X_test)\n",
    "test_score = r2_score(y_test,y_pred)\n",
    "accuracyResult = metrics.accuracy_score(y_test,y_pred)\n",
    "print(\"R2 Score: \",test_score)\n",
    "print(\"Accuracy Score: \",accuracyResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a transform to normalize the data\n",
    "# transform = transforms.Compose([transforms.ToTensor(),\n",
    "#                                 transforms.Normalize((0.5,), (0.5,)),\n",
    "#                               ])\n",
    "# Download and load the training data\n",
    "NN_X_train = torch.tensor(df.drop([\"label\"], axis=1).values)\n",
    "NN_y_train = torch.tensor(df[\"label\"].values)\n",
    "trainset = TensorDataset(NN_X_train, NN_y_train)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 20\n",
    "NUM_HIDDEN1_NODES = 400\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "model = nn.Sequential(nn.Linear(NUM_FEATURES, NUM_HIDDEN1_NODES),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(NUM_HIDDEN1_NODES, 1)\n",
    "                     )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    for data, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
    "    \n",
    "# class NeuralNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNet, self).__init__()\n",
    "        \n",
    "#         # Inputs to hidden layer linear transformation\n",
    "#         self.hidden1 = nn.Linear(NUM_FEATURES, NUM_HIDDEN1_NODES)\n",
    "#         self.output = nn.Linear(NUM_HIDDEN1_NODES, 1)\n",
    "        \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.hidden1(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         x = self.output(x)\n",
    "#         x = self.softmax(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Network()\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Cross-Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "sample_model_kfold = sample()\n",
    "\n",
    "sample_results_kfold = model_selection.cross_val_score(sample_model_kfold, df_prepared, df_labels, cv=kfold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
